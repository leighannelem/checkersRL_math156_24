{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from state import State\n",
    "from players import Player, HumanPlayer, RandomPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with Human (Untrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Player(\"computer\", exp_rate=1) # fully random\n",
    "p2 = HumanPlayer(\"human\")\n",
    "test_state = State(p1, p2)\n",
    "test_state.play_human()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: fix winner so that can accurately check if there are any legal moves left... i think it should work now\n",
    "--> maybe replace with an almost-won starting board just so can test\n",
    "\n",
    "TODO: how to evaluate performance of the agent?\n",
    "--> maybe introduce another method and compare them?\n",
    "idk need to do some research on this..\n",
    "\n",
    "TODO: track \"learning curves\"\n",
    "plot cumulative reward, average reward per episode, numper of steps per episode, success rate over time\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "Rounds 0\n",
      "Rounds 240\n",
      "Rounds 480\n",
      "Rounds 720\n",
      "Rounds 960\n",
      "Rounds 1200\n",
      "Rounds 1440\n",
      "Rounds 1680\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m st \u001b[38;5;241m=\u001b[39m State(p1, p2)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m12000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arnav\\OneDrive\\Desktop\\Math 156\\156_final_project\\checkersRL_math156_24\\state.py:221\u001b[0m, in \u001b[0;36mState.play\u001b[1;34m(self, rounds)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# Player 2 takes their turn\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     moves \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailableMoves()\n\u001b[1;32m--> 221\u001b[0m     p2_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchooseAction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmoves\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mboard\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplayerSymbol\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# take action and update board state\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdateState(p2_action)\n",
      "File \u001b[1;32mc:\\Users\\arnav\\OneDrive\\Desktop\\Math 156\\156_final_project\\checkersRL_math156_24\\players.py:52\u001b[0m, in \u001b[0;36mPlayer.chooseAction\u001b[1;34m(self, moves, current_board, symbol)\u001b[0m\n\u001b[0;32m     49\u001b[0m     next_board[(start_row \u001b[38;5;241m+\u001b[39m end_row) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m][(start_col \u001b[38;5;241m+\u001b[39m end_col) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# no double jumps (for now) <3\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m next_boardHash \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetHash\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_board\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates_value\u001b[38;5;241m.\u001b[39mget(next_boardHash) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\arnav\\OneDrive\\Desktop\\Math 156\\156_final_project\\checkersRL_math156_24\\players.py:18\u001b[0m, in \u001b[0;36mPlayer.getHash\u001b[1;34m(self, board)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetHash\u001b[39m(\u001b[38;5;28mself\u001b[39m, board):\n\u001b[1;32m---> 18\u001b[0m     boardHash \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mboard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m boardHash\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "p1 = Player(\"p1\")\n",
    "p2 = Player(\"p2\")\n",
    "\n",
    "st = State(p1, p2)\n",
    "print(\"training...\")\n",
    "st.play(12000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1.savePolicy()\n",
    "p2.savePolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human vs. Computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Player(\"computer\", exp_rate=0) #takes random action 0% of time\n",
    "p1.loadPolicy(\"policy_p1\")\n",
    "\n",
    "p2 = HumanPlayer(\"human\")\n",
    "\n",
    "st = State(p1, p2)\n",
    "st.play_human() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
